import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Load datasets
df1 = pd.read_csv('/Users/atrabaja/Documents/FinMark_MachineLearning/customernew.csv', encoding='ISO-8859-1')
df2 = pd.read_csv('/Users/atrabaja/Documents/FinMark_MachineLearning/productnew.csv', encoding='ISO-8859-1')
df3 = pd.read_csv('/Users/atrabaja/Documents/FinMark_MachineLearning/transactionsnew.csv', encoding='ISO-8859-1')

# Step 1: Data Preprocessing

# 1.1 Handle missing values
# Customer Dataset - Impute missing Company_Profit with median
df1['Company_Profit'] = df1['Company_Profit'].fillna(df1['Company_Profit'].median())

# Product Dataset - Drop rows with missing Product_ID and clean Product_Price
df2 = df2.dropna(subset=['Product_ID'])
df2['Product_Price'] = df2['Product_Price'].str.replace(',', '').astype(float)

# Transactions Dataset - Clean Quantity and drop rows with critical missing values
df3['Quantity'] = pd.to_numeric(df3['Quantity'], errors='coerce')
df3 = df3.dropna(subset=['Transaction_ID', 'Company_ID', 'Product_ID', 'Product_Price', 'Total_Cost'])

# 1.2 Convert Transaction_Date to datetime format
df3['Transaction_Date'] = pd.to_datetime(df3['Transaction_Date'], format='%d-%m-%Y')

# Step 2: Exploratory Data Analysis (EDA)

# Checking structure and previewing data
print("Customer Dataset Info:")
print(df1.info())
print(df1.head())

print("\nProduct Dataset Info:")
print(df2.info())
print(df2.head())

print("\nTransactions Dataset Info:")
print(df3.info())
print(df3.head())

# Checking for missing values
print("\nMissing Values in Customer Dataset:")
print(df1.isnull().sum())

print("\nMissing Values in Product Dataset:")
print(df2.isnull().sum())

print("\nMissing Values in Transactions Dataset:")
print(df3.isnull().sum())

# Visualizing distribution of Company Profit
plt.figure(figsize=(10, 6))
sns.histplot(df1['Company_Profit'], bins=30, kde=True)
plt.title('Distribution of Company Profit')
plt.xlabel('Company Profit')
plt.ylabel('Frequency')
plt.show()

# Step 3: Feature Engineering

# 3.1 Merge datasets
merged_df = df3.merge(df1, on='Company_ID', how='left')
merged_df = merged_df.merge(df2, on='Product_ID', how='left')

# 3.2 Create new features
# Total Transactions per Customer
customer_transactions = merged_df.groupby('Company_ID').agg({
    'Transaction_ID': 'count',
    'Total_Cost': 'sum',
    'Transaction_Date': 'max'
}).rename(columns={'Transaction_ID': 'Total_Transactions', 'Total_Cost': 'Total_Spent'})

# Average Purchase Value
customer_transactions['Avg_Purchase_Value'] = customer_transactions['Total_Spent'] / customer_transactions['Total_Transactions']

# Recency (days since last purchase)
latest_date = merged_df['Transaction_Date'].max()
customer_transactions['Recency'] = (latest_date - customer_transactions['Transaction_Date']).dt.days

# Merge features back with customer data
final_df = df1.merge(customer_transactions, on='Company_ID', how='left').fillna(0)

# 3.3 Define target variable
# If a customer made a purchase in the last 180 days, label as 1, else 0
final_df['Purchased_Recently'] = np.where(final_df['Recency'] <= 180, 1, 0)

# Step 4: Correlation Analysis

# Correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(final_df[['Company_Profit', 'Total_Spent', 'Avg_Purchase_Value', 'Total_Transactions', 'Recency']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Visualizing purchase behavior
plt.figure(figsize=(8, 5))
sns.countplot(x='Purchased_Recently', data=final_df)
plt.title('Purchase Behavior Distribution')
plt.xlabel('Purchased Recently (1 = Yes, 0 = No)')
plt.ylabel('Count')
plt.show()

# Step 5: Model Selection and Training

# 5.1 Prepare features and target
features = final_df[['Company_Profit', 'Total_Transactions', 'Total_Spent', 'Avg_Purchase_Value', 'Recency', 'City']]
target = final_df['Purchased_Recently']

# One-hot encode categorical feature 'City'
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), ['Company_Profit', 'Total_Transactions', 'Total_Spent', 'Avg_Purchase_Value', 'Recency']),
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['City'])
    ])

# Define the model pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# 5.2 Split data into train and test sets using stratified sampling
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.3, random_state=42, stratify=target
)

# 5.3 Train the model
pipeline.fit(X_train, y_train)

# Step 6: Model Evaluation

# 6.1 Make predictions
y_pred = pipeline.predict(X_test)

# Check if model supports probability prediction for both classes
if len(np.unique(y_test)) == 2:
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Probability for the positive class

    # 6.2 Evaluate model performance
    print("Classification Report:\n", classification_report(y_test, y_pred, labels=[0, 1], zero_division=0))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred, labels=[0, 1]))
    print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_proba))
else:
    print("Only one class present in y_test. ROC-AUC Score cannot be calculated.")
    print("Classification Report:\n", classification_report(y_test, y_pred, labels=[0, 1], zero_division=0))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred, labels=[0, 1]))

# Additional Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy Score: {accuracy:.2f}')

# 6.3 Feature importance analysis (for Random Forest)
feature_names = (
    pipeline.named_steps['preprocessor'].transformers_[0][2] + 
    list(pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(['City']))
)
feature_importances = pipeline.named_steps['classifier'].feature_importances_

# Display feature importance
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)
print(feature_importance_df)

# Step 7: Conclusion
# The RandomForestClassifier provides insights into customer purchase likelihood based on recency and spending behavior. Performance metrics guide further feature engineering or hyperparameter tuning to improve model accuracy.
